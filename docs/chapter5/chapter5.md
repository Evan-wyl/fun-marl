# 多智能体强化学习简介



## 算法架构

## Markov Game



## Extensive-form Game



## 学习场景

多智能体强化学习常见场景有**合作场景**、**竞争场景**、**混合场景**。其中，合作场景是指智能体之间处于合作关系；竞争场景是指智能体之间处于竞争关系；混合场景是指智能体之间既有合作关系，也有竞争关系，例如：足球运动中队员之间的关系。

合作场景中，智能体共享相同的奖励函数，即所有智能体具有共同的累积回报、Q函数、价值函数。

竞争场景中，智能体之间被建模为**零和博弈**。在零和博弈场景中，一个智能体的奖励就是另一个智能体的损失，也$\forall(s,a,{s}')$，都有$\frac{1}{N}\sum_{i\epsilon N}R^i (s,a,{s}')=0$。为了降低算法分析的难度，该场景下的智能体个数常常只有两个。

混合场景中，每个智能体是自私自利的，它们的目的和关系无限制。也可以理解为，它们的目标可能会与其它智能体产生冲突，也可能具有相同的目标。在该场景中，**博弈论**中均衡的概念对算法开发有很大的影响，例如：**纳什均衡**。



## 挑战

### 非唯一的学习目标



### 非静态



### 扩展性问题



### 各种各样的信息结构



#### 智能体的信息结构

由于部分可观测的环境会加剧非静态对智能体学习的影响，因此不同的学习和执行方式会产生各种各样的信息结构，分别有**中心化结构**、**带有网络智能体的去中心化结构**、**完全去中心化结构**。



## 算法的类别





## 参考文献

